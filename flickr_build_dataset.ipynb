{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d57097-50c1-47e2-9806-6e56dcb36fb6",
   "metadata": {},
   "source": [
    "# Build a dataset with the flickr API\n",
    "\n",
    "API reference: https://www.flickr.com/services/api/ <br>\n",
    "Python API: https://github.com/sybrenstuvel/flickrapi <br>\n",
    "API allows for 3600 calls per hour.\n",
    "\n",
    "The quickest way to create a hand-picked dataset is probably to download the pictures manually and use the ids_from_files() function bellow to get the image ids from the file names. Alternatively, create a gallery directly on flickr and get your ids over the API from there (the problem here is that some creators do not allow the adding to galleries, so it won't cover all pictures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cceb93-82a3-4411-8da7-4e1cc3be2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import cfg\n",
    "import os\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from pprint import pprint as _pprint\n",
    "def pprint(data): (_pprint(data, sort_dicts=False))\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import flickrapi\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "KEY = cfg.CRED.FLICKR_KEY\n",
    "SECRET = cfg.CRED.FLICKR_SECRET\n",
    "flickr = flickrapi.FlickrAPI(KEY, SECRET)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b7b67d8f-7eab-43c0-a80b-081deacf8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build constants for annotation file\n",
    "\n",
    "# dataset info; json key is 'info'\n",
    "INFO = {\n",
    "    \"description\": \"Dataset\",\n",
    "    \"url\": \"https://github.com/random9v2/cv-annot\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"year\": 2022,\n",
    "    \"contributor\": \"John\",\n",
    "    \"date_created\": \"2022/08/10\"\n",
    "}\n",
    "\n",
    "# get flickr licenses, order, remove no CC license ('id':0); json key is 'licenses'\n",
    "LICENSES = sorted(json.loads(flickr.photos.licenses.getInfo(format=\"json\"))[\"licenses\"][\"license\"], key=lambda d: d['id'])[1:]\n",
    "\n",
    "# original COCO categories for human keypoints; json key is 'categories'\n",
    "CATEGORIES_COCO = [\n",
    "        {\n",
    "            \"supercategory\": \"person\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"person\",\n",
    "            \"keypoints\": [ # keypoint keys\n",
    "                \"nose\",\n",
    "                \"left_eye\",\n",
    "                \"right_eye\",\n",
    "                \"left_ear\",\n",
    "                \"right_ear\",\n",
    "                \"left_shoulder\",\n",
    "                \"right_shoulder\",\n",
    "                \"left_elbow\",\n",
    "                \"right_elbow\",\n",
    "                \"left_wrist\",\n",
    "                \"right_wrist\",\n",
    "                \"left_hip\",\n",
    "                \"right_hip\",\n",
    "                \"left_knee\",\n",
    "                \"right_knee\",\n",
    "                \"left_ankle\",\n",
    "                \"right_ankle\"\n",
    "            ],\n",
    "            \"skeleton\": [ # how keypoints are connected for visualization; does not affect training\n",
    "                [16,14],\n",
    "                [14,12],\n",
    "                [17,15],\n",
    "                [15,13],\n",
    "                [12,13],\n",
    "                [6,12],\n",
    "                [7,13],\n",
    "                [6,7],\n",
    "                [6,8],\n",
    "                [7,9],\n",
    "                [8,10],\n",
    "                [9,11],\n",
    "                [2,3],\n",
    "                [1,2],\n",
    "                [1,3],\n",
    "                [2,4],\n",
    "                [3,5],\n",
    "                [4,6],\n",
    "                [5,7]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# original crowd_pose categories for human keypoints; json key is 'categories'\n",
    "CATEGORIES_CROWD = [\n",
    "        {\n",
    "            \"supercategory\": \"person\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"person\",\n",
    "            \"keypoints\": [\n",
    "                \"left_shoulder\",\n",
    "                \"right_shoulder\",\n",
    "                \"left_elbow\",\n",
    "                \"right_elbow\",\n",
    "                \"left_wrist\",\n",
    "                \"right_wrist\",\n",
    "                \"left_hip\",\n",
    "                \"right_hip\",\n",
    "                \"left_knee\",\n",
    "                \"right_knee\",\n",
    "                \"left_ankle\",\n",
    "                \"right_ankle\",\n",
    "                \"head\",\n",
    "                \"neck\"\n",
    "            ],\n",
    "            \"skeleton\": [\n",
    "                [1,14],\n",
    "                [1,3],\n",
    "                [2,14],\n",
    "                [2,4],\n",
    "                [3,5],\n",
    "                [4,6],\n",
    "                [7,14],\n",
    "                [7,9],\n",
    "                [8,14],\n",
    "                [8,10],\n",
    "                [9,11],\n",
    "                [10,12],\n",
    "                [13,14]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "# with open(f\"configs/crowdpose_keypoint_cat.json\", 'w', encoding='utf-8') as f:\n",
    "#         json.dump({\"categories\": CATEGORIES_CROWD}, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcc2fab-ba43-47a8-bf06-9b001772b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(LICENSES)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8ccd6915-1954-4605-a371-6e0e88348379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build/add to dataset\n",
    "# with big datasets (> 1800), the function must be scheduled to not exeed the hourly API quota (easiest is a sleep() between each image)\n",
    "\n",
    "ID_PREFIX = '007'  # file name and ID prefix for this dataset (only decorative/helps differentiate sets)\n",
    "CATEGORIES = CATEGORIES_CROWD  # coco or crowd_pose\n",
    "\n",
    "def build_dataset(flickr_ids:list, img_dir:str, anno_file:str=None, update:dict=None):\n",
    "    \"\"\"\n",
    "    Builds dataset with annotation file out of flickr image IDs.\n",
    "    New images will be added to a given annotation file. Old annotations won't be altered.\n",
    "    \n",
    "    args:\n",
    "    flickr_ids: list of flickr image ids\n",
    "    img_dir: path to download the new images into (dataset path)\n",
    "    anno_file (optional): annotation file to add new images to\n",
    "    update / NOT IMPLEMENTED: update part of the annotation file, e.g. {\"info\": {}, \"licenses\": {}}; due to safety, updating images/annotations should be disabled\n",
    "    \"\"\"    \n",
    "    final_anno = {}\n",
    "    max_id = max_count_id = 0\n",
    "    flickr_ids_add = set(flickr_ids)  # add to anno\n",
    "    flickr_ids_download = set() # download\n",
    "    version = '1.0'\n",
    "    changes = False\n",
    "    \n",
    "    if anno_file:\n",
    "        with open(anno_file) as f:\n",
    "            anno_json = json.load(f)\n",
    "            \n",
    "        # copy of original\n",
    "        original_anno = anno_json.copy()\n",
    "        \n",
    "        # increase version number if images are added\n",
    "        version = f'{(int(float(anno_json[\"info\"][\"version\"])) + 1):.1f}'\n",
    "    \n",
    "        # only add new flickr images\n",
    "        ids_in_json = [(image[\"flickr_id\"] if image.get(\"flickr_id\", False) else '') for image in anno_json[\"images\"]]\n",
    "        flickr_ids_add = set(flickr_ids) - set(ids_in_json)\n",
    "        \n",
    "        # for downloading what isn't there\n",
    "        imgs_in_dir = set([x.name for x in Path(img_dir).glob(\"*.jpg\")])\n",
    "        for image in anno_json[\"images\"]:\n",
    "            if image[\"file_name\"] not in imgs_in_dir:\n",
    "                flickr_ids_download.add(image[\"flickr_id\"])\n",
    "        \n",
    "        # get biggest dataset id\n",
    "        max_id = max(anno_json[\"images\"], key=lambda x:x[\"id\"])[\"id\"]\n",
    "        max_count_id = int(str(max_id)[1:])\n",
    "        \n",
    "        final_anno = anno_json\n",
    "    else:\n",
    "        # build starter annotation file with header\n",
    "        final_anno[\"info\"] = INFO\n",
    "        final_anno[\"licenses\"] = LICENSES\n",
    "        final_anno[\"categories\"] = CATEGORIES\n",
    "        final_anno[\"images\"] = []\n",
    "            \n",
    "    # build image meta for all new ids\n",
    "    image_meta = []\n",
    "    allowed_licenses = [x[\"id\"] for x in final_anno[\"licenses\"]]\n",
    "    count_id = 1 if max_count_id==0 else max_count_id+1000  # keep dataset image ids between versions appart\n",
    "    \n",
    "    pbar = tqdm(flickr_ids_add)\n",
    "    for flickr_id in pbar:\n",
    "        file_name = f'{ID_PREFIX}{str(count_id).zfill(9)}.jpg'\n",
    "        response = json.loads(flickr.photos.getInfo(photo_id=flickr_id, format=\"json\"))\n",
    "        if response.get(\"photo\", True):\n",
    "            if int(response[\"photo\"][\"license\"]) in allowed_licenses:\n",
    "                url = get_url(flickr_id, 'z')\n",
    "                download_image(url, img_dir, file_name)\n",
    "                remove_metadata_img(Path(img_dir, file_name), quality=98)\n",
    "                height, width = Image.open(Path(img_dir, file_name)).size\n",
    "                image_meta.append(\n",
    "                    {\n",
    "                        \"license\": response[\"photo\"][\"license\"],\n",
    "                        \"file_name\": file_name,\n",
    "                        \"dataset_version\": version,\n",
    "                        \"height\": height,\n",
    "                        \"width\": width,\n",
    "                        \"date_captured\": response[\"photo\"][\"dates\"][\"taken\"],\n",
    "                        \"flickr_url\": url,\n",
    "                        \"content_url\": response[\"photo\"][\"urls\"][\"url\"][0][\"_content\"],\n",
    "                        \"flickr_id\": response[\"photo\"][\"id\"],\n",
    "                        \"id\": int(f'{ID_PREFIX}{str(count_id).zfill(9)}')\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                print(f'License Error: image {flickr_id} not added; license {response[\"photo\"][\"license\"]} not allowed.')\n",
    "        else:\n",
    "            print(f'Error for flickr_id{flickr_id}: json response does not contain a photo item \\n json response: \\n')\n",
    "            pprint(response)   \n",
    "        count_id+=1\n",
    "        pbar.set_description(f'download+annotate: image: {flickr_id}')\n",
    "        \n",
    "    changes = True if len(image_meta) > 0 else False\n",
    "    final_anno[\"images\"] += image_meta\n",
    "\n",
    "    # download remaining\n",
    "    if len(flickr_ids_download) > 0:\n",
    "        pbar_2 = tqdm(flickr_ids_download) \n",
    "        for flickr_id in pbar_2:\n",
    "            file_name = next((item for item in final_anno[\"images\"] if item[\"flickr_id\"] == flickr_id), None)[\"file_name\"]\n",
    "            url = get_url(flickr_id, 'z')\n",
    "            download_image(url, img_dir, file_name)\n",
    "            remove_metadata_img(Path(img_dir, file_name))\n",
    "            pbar_2.set_description(f'download: image: {flickr_id}')\n",
    "    \n",
    "    # finish and save annotation file\n",
    "    final_anno[\"info\"][\"version\"] = version\n",
    "    save_file = f'annotationMeta_v{version}.json'\n",
    "    if anno_file:\n",
    "        if changes:\n",
    "            save_path = Path(Path(anno_file).parent, save_file)\n",
    "            if save_path.is_file():  \n",
    "                print(f'Error: {save_file} already exists.')\n",
    "            else:\n",
    "                with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(final_anno, f, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print('Warning: annotation file says: nothing changed')\n",
    "    else:\n",
    "        save_path = Path(img_dir, save_file)\n",
    "        if save_path.is_file():  \n",
    "            print(f'Error: {save_file} already exists in image directory.')\n",
    "        else:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(final_anno, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "\n",
    "## helper functions\n",
    "\n",
    "def get_url(id:str, size_suffix:str=''):\n",
    "    \"\"\"Gets URL for given image id and size.\n",
    "\n",
    "    args:\n",
    "    id: flickr image id\n",
    "    size: defines image size with suffix: https://www.flickr.com/services/api/misc.urls.html\n",
    "    \"\"\"\n",
    "    get_sizes = json.loads(flickr.photos.getSizes(photo_id=id, format=\"json\"))\n",
    "    found = False\n",
    "    last_size_url = None\n",
    "    for size in get_sizes['sizes']['size']:\n",
    "        last_size_url = size['source']\n",
    "        _size_suffix = size['source'].split('_')[-1].split('.')[0]  # gets (size) suffix from image source link\n",
    "        if _size_suffix == size_suffix:\n",
    "            url = size['source']\n",
    "            found = True\n",
    "            break\n",
    "        elif len(_size_suffix) > 1 and size_suffix == '':  # standard format without suffix in link\n",
    "            url = size['source']\n",
    "            found = True\n",
    "            break\n",
    "    if not found:   \n",
    "        url = last_size_url\n",
    "        suffix = last_size_url.split('_')[-1].split('.')[0]\n",
    "        print(f'Link/size not found for {id}; biggest image URL available saved, suffix: {suffix if len(suffix)==1 else \"none (default)\"}')\n",
    "    return url\n",
    "\n",
    "def download_image(url:str, output_dir:str, file_name:str):\n",
    "    \"\"\"Donwloads image from URL.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, Path(output_dir, file_name))\n",
    "    \n",
    "def remove_metadata_img(img_path:str, quality=75):\n",
    "    \"\"\"Removes all metadata of an image. Will also compress the image (PIL default is quality=75).\n",
    "    \"\"\"\n",
    "    image = Image.open(img_path)\n",
    "    data = list(image.getdata())\n",
    "    img_no_meta = Image.new(image.mode, image.size)\n",
    "    img_no_meta.putdata(data)\n",
    "    img_no_meta.save(img_path, quality=quality)  # default quality=75\n",
    "\n",
    "def ids_from_files(dir):\n",
    "    \"\"\"Retrieve flickr IDs from the file name of downloaded images.\n",
    "    \n",
    "    flickr file name format: numericalID_imageSecret_imageSize.jpg, e.g. 50235154168_b3201cd930_o.jpg\n",
    "    \"\"\"\n",
    "    return set([x.name.split('_')[0] for x in Path(dir).glob(\"*.jpg\")])\n",
    "\n",
    "\n",
    "# ids = ids_from_files('/Users/john/Downloads/fireground_dataset_raw')\n",
    "# test_ids = ['4882517696', '4882517696']\n",
    "# build_dataset(ids, '/Users/john/Downloads/fireground_dataset', None)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "205d31fd-8cd9-422b-afc1-016dc9acf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After annotating in coco-annotator, add the annotations to the annotation file created above\n",
    "\n",
    "# keys in annotations to be removed\n",
    "ANNO_DELETE = [\"color\", \"metadata\"]\n",
    "\n",
    "\n",
    "def add_coco_annotations(anno_meta, anno_coco):\n",
    "    \"\"\"\n",
    "    Adds in external tools created annotations to the annotation file created above. \n",
    "    The file names must remain the same during the whole process.\n",
    "    Image ids are drawn from the anno_meta file.\n",
    "    \n",
    "    args:\n",
    "    anno_meta: path to .json file – contains meta information for a dataset, e.g., created above with build dataset\n",
    "    anno_coco: path to .json file – annotation export from coco-annotator\n",
    "    \"\"\"\n",
    "    with open(anno_meta) as f:\n",
    "        meta_json = json.load(f)\n",
    "    with open(anno_coco) as f:\n",
    "        coco_json = json.load(f)\n",
    "        \n",
    "    # build lookup tables with file names and image ids\n",
    "    key_fn_id_meta = {}\n",
    "    key_id_fn_coco = {}\n",
    "    for image in meta_json[\"images\"]:\n",
    "        key_fn_id_meta[image[\"file_name\"]] = image[\"id\"]\n",
    "    for image in coco_json[\"images\"]:\n",
    "        key_id_fn_coco[image[\"id\"]] = image[\"file_name\"]\n",
    "    # keep category id from meta_json\n",
    "    # category name has to be the same\n",
    "    key_name_cat_meta = {}\n",
    "    key_cat_name_coco = {}\n",
    "    for category in meta_json[\"categories\"]:\n",
    "        key_name_cat_meta[category[\"name\"]] = category[\"id\"]\n",
    "    for category in coco_json[\"categories\"]:\n",
    "        key_cat_name_coco[category[\"id\"]] = category[\"name\"]\n",
    "        \n",
    "    annotations = []\n",
    "    for anno in coco_json[\"annotations\"]:\n",
    "        file_name = key_id_fn_coco[anno[\"image_id\"]]\n",
    "        id_meta = key_fn_id_meta[file_name]\n",
    "        category_name = key_cat_name_coco[anno[\"category_id\"]]\n",
    "        category_id_meta = key_name_cat_meta[category_name]\n",
    "        \n",
    "        # annotation items to keep\n",
    "        annotation = {}\n",
    "        for key, value in anno.items():\n",
    "            if key not in ANNO_DELETE:\n",
    "                annotation[key] = value\n",
    "        \n",
    "        # overwrite id keys with anno_meta values\n",
    "        annotation[\"id\"] = anno[\"id\"]\n",
    "        annotation[\"image_id\"] = id_meta\n",
    "        annotation[\"category_id\"] = category_id_meta\n",
    "        \n",
    "        # finally add new annotation\n",
    "        annotations.append(annotation)\n",
    "        \n",
    "    # merge and save\n",
    "    final_anno = meta_json.copy()\n",
    "    final_anno[\"annotations\"] = annotations\n",
    "    version = meta_json[\"info\"][\"version\"]\n",
    "    save_file = f'annotations_v{version}.json'\n",
    "    save_path = Path(Path(anno_meta).parent, save_file)\n",
    "    if save_path.is_file():  \n",
    "        print(f'Error: {save_file} already exists.')\n",
    "    else:\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_anno, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "            \n",
    "# add_coco_annotations('/Users/john/Downloads/test/annotationMeta_v1.0.json', '/Users/john/Downloads/test/test-4.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d007d9b-ac33-4b33-a417-b0693f7454aa",
   "metadata": {},
   "source": [
    "# some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cac2e16-628c-4622-916b-80f519556e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_key(container:dict):\n",
    "    \"\"\"Drop dict key&value on all levels; e.g., segmentations in an annotation file.\n",
    "    \"\"\"\n",
    "    if not isinstance(container, dict):\n",
    "        return container if not isinstance(container, list) else list(map(d_rem, container))\n",
    "    return {a:d_rem(b) for a, b in container.items() if a != 'segmentation'}  # adjust key\n",
    "\n",
    "\n",
    "json_path = '/Users/john/git/_tools/coco-annotator/datasets/annotations_coco_person/coco_person_keypoints_val2017_segDel.json'\n",
    "save_path = '/Users/john/git/_tools/coco-annotator/datasets/annotations_coco_person/coco_person_keypoints_val2017_segDel2.json'\n",
    "\n",
    "# with open(json_path) as f:\n",
    "#     b = remove_key(json.load(f))\n",
    "# with open(save_path, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(b, f, ensure_ascii=False, indent=4)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
