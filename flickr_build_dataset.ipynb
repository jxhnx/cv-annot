{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d5d57097-50c1-47e2-9806-6e56dcb36fb6",
   "metadata": {},
   "source": [
    "# Build a dataset with the flickr API\n",
    "\n",
    "API reference: https://www.flickr.com/services/api/ <br>\n",
    "Python API: https://github.com/sybrenstuvel/flickrapi <br>\n",
    "API allows for 3600 calls per hour.\n",
    "\n",
    "The quickest way to create a hand-picked dataset is probably to download the pictures manually and use the ids_from_files() function bellow to get the image ids from the file names. Alternatively, create a gallery directly on flickr and get your ids over the API from there (the problem here is that some creators do not allow the adding to galleries, so it won't cover all pictures)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "00cceb93-82a3-4411-8da7-4e1cc3be2674",
   "metadata": {},
   "outputs": [],
   "source": [
    "from configs import cfg\n",
    "import os\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "import glob\n",
    "import json\n",
    "from pprint import pprint as _pprint\n",
    "def pprint(data): (_pprint(data, sort_dicts=False))\n",
    "from PIL import Image\n",
    "import urllib.request\n",
    "import flickrapi\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "KEY = cfg.CRED.FLICKR_KEY\n",
    "SECRET = cfg.CRED.FLICKR_SECRET\n",
    "flickr = flickrapi.FlickrAPI(KEY, SECRET)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cab69e6-ab12-4a43-ac9f-3e674dc13358",
   "metadata": {},
   "source": [
    "## Define annotation file header"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "b7b67d8f-7eab-43c0-a80b-081deacf8be5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build constants for annotation file\n",
    "\n",
    "# dataset info; json key is 'info'\n",
    "INFO = {\n",
    "    \"description\": \"Dataset\",\n",
    "    \"url\": \"https://github.com/random9v2/cv-annot\",\n",
    "    \"version\": \"1.0\",\n",
    "    \"year\": 2022,\n",
    "    \"contributor\": \"John\",\n",
    "    \"date_created\": \"2022/08/10\"\n",
    "}\n",
    "\n",
    "# get flickr licenses, order, remove no CC license ('id':0); json key is 'licenses'\n",
    "LICENSES = sorted(json.loads(flickr.photos.licenses.getInfo(format=\"json\"))[\"licenses\"][\"license\"], key=lambda d: d['id'])[1:]\n",
    "\n",
    "# original COCO categories for human keypoints; json key is 'categories'\n",
    "CATEGORIES_COCO = [\n",
    "        {\n",
    "            \"supercategory\": \"person\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"person\",\n",
    "            \"keypoints\": [ # keypoint keys\n",
    "                \"nose\",\n",
    "                \"left_eye\",\n",
    "                \"right_eye\",\n",
    "                \"left_ear\",\n",
    "                \"right_ear\",\n",
    "                \"left_shoulder\",\n",
    "                \"right_shoulder\",\n",
    "                \"left_elbow\",\n",
    "                \"right_elbow\",\n",
    "                \"left_wrist\",\n",
    "                \"right_wrist\",\n",
    "                \"left_hip\",\n",
    "                \"right_hip\",\n",
    "                \"left_knee\",\n",
    "                \"right_knee\",\n",
    "                \"left_ankle\",\n",
    "                \"right_ankle\"\n",
    "            ],\n",
    "            \"skeleton\": [ # how keypoints are connected for visualization; does not affect training\n",
    "                [16,14],\n",
    "                [14,12],\n",
    "                [17,15],\n",
    "                [15,13],\n",
    "                [12,13],\n",
    "                [6,12],\n",
    "                [7,13],\n",
    "                [6,7],\n",
    "                [6,8],\n",
    "                [7,9],\n",
    "                [8,10],\n",
    "                [9,11],\n",
    "                [2,3],\n",
    "                [1,2],\n",
    "                [1,3],\n",
    "                [2,4],\n",
    "                [3,5],\n",
    "                [4,6],\n",
    "                [5,7]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "# original crowd_pose categories for human keypoints; json key is 'categories'\n",
    "CATEGORIES_CROWD = [\n",
    "        {\n",
    "            \"supercategory\": \"person\",\n",
    "            \"id\": 1,\n",
    "            \"name\": \"person\",\n",
    "            \"keypoints\": [\n",
    "                \"left_shoulder\",\n",
    "                \"right_shoulder\",\n",
    "                \"left_elbow\",\n",
    "                \"right_elbow\",\n",
    "                \"left_wrist\",\n",
    "                \"right_wrist\",\n",
    "                \"left_hip\",\n",
    "                \"right_hip\",\n",
    "                \"left_knee\",\n",
    "                \"right_knee\",\n",
    "                \"left_ankle\",\n",
    "                \"right_ankle\",\n",
    "                \"head\",\n",
    "                \"neck\"\n",
    "            ],\n",
    "            \"skeleton\": [\n",
    "                [1,14],\n",
    "                [1,3],\n",
    "                [2,14],\n",
    "                [2,4],\n",
    "                [3,5],\n",
    "                [4,6],\n",
    "                [7,14],\n",
    "                [7,9],\n",
    "                [8,14],\n",
    "                [8,10],\n",
    "                [9,11],\n",
    "                [10,12],\n",
    "                [13,14]\n",
    "            ]\n",
    "        }\n",
    "    ]\n",
    "\n",
    "\n",
    "# with open(f\"configs/crowdpose_keypoint_cat.json\", 'w', encoding='utf-8') as f:\n",
    "#         json.dump({\"categories\": CATEGORIES_CROWD}, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "3dcc2fab-ba43-47a8-bf06-9b001772b26a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pprint(LICENSES)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d102035-d6c9-4139-bd49-0605694c1f9d",
   "metadata": {},
   "source": [
    "## Build or expand annotation file with flickr image ids (add metadata)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "8ccd6915-1954-4605-a371-6e0e88348379",
   "metadata": {},
   "outputs": [],
   "source": [
    "# build/add to dataset\n",
    "# with big datasets (> 1800), the function must be scheduled to not exeed the hourly API quota (easiest is a sleep() between each image)\n",
    "\n",
    "ID_PREFIX = '007'  # file name and ID prefix for this dataset (only decorative/helps differentiate sets)\n",
    "CATEGORIES = CATEGORIES_CROWD  # coco or crowd_pose\n",
    "\n",
    "def build_dataset(flickr_ids:list, img_dir:str, anno_file:str=None, update:dict=None):\n",
    "    \"\"\"\n",
    "    Builds dataset with annotation file out of flickr image IDs.\n",
    "    New images will be added to a given annotation file. Old annotations won't be altered.\n",
    "    \n",
    "    args:\n",
    "    flickr_ids: list of flickr image ids\n",
    "    img_dir: path to download the new images into (dataset path)\n",
    "    anno_file (optional): annotation file to add new images to\n",
    "    update / NOT IMPLEMENTED: update part of the annotation file, e.g. {\"info\": {}, \"licenses\": {}}; due to safety, updating images/annotations should be disabled\n",
    "    \"\"\"    \n",
    "    final_anno = {}\n",
    "    max_id = max_count_id = 0\n",
    "    flickr_ids_add = set(flickr_ids)  # add to anno\n",
    "    flickr_ids_download = set() # download\n",
    "    version = '1.0'\n",
    "    changes = False\n",
    "    \n",
    "    if anno_file:\n",
    "        with open(anno_file) as f:\n",
    "            anno_json = json.load(f)\n",
    "            \n",
    "        # copy of original\n",
    "        original_anno = anno_json.copy()\n",
    "        \n",
    "        # increase version number if images are added\n",
    "        version = f'{(int(float(anno_json[\"info\"][\"version\"])) + 1):.1f}'\n",
    "    \n",
    "        # only add new flickr images\n",
    "        ids_in_json = [(image[\"flickr_id\"] if image.get(\"flickr_id\", False) else '') for image in anno_json[\"images\"]]\n",
    "        flickr_ids_add = set(flickr_ids) - set(ids_in_json)\n",
    "        \n",
    "        # for downloading what isn't there\n",
    "        imgs_in_dir = set([x.name for x in Path(img_dir).glob(\"*.jpg\")])\n",
    "        for image in anno_json[\"images\"]:\n",
    "            if image[\"file_name\"] not in imgs_in_dir:\n",
    "                flickr_ids_download.add(image[\"flickr_id\"])\n",
    "        \n",
    "        # get biggest dataset id\n",
    "        max_id = max(anno_json[\"images\"], key=lambda x:x[\"id\"])[\"id\"]\n",
    "        max_count_id = int(str(max_id)[1:])\n",
    "        \n",
    "        final_anno = anno_json\n",
    "    else:\n",
    "        # build starter annotation file with header\n",
    "        final_anno[\"info\"] = INFO\n",
    "        final_anno[\"licenses\"] = LICENSES\n",
    "        final_anno[\"categories\"] = CATEGORIES\n",
    "        final_anno[\"images\"] = []\n",
    "            \n",
    "    # build image meta for all new ids\n",
    "    image_meta = []\n",
    "    allowed_licenses = [x[\"id\"] for x in final_anno[\"licenses\"]]\n",
    "    count_id = 1 if max_count_id==0 else max_count_id+1_000_001  # keep dataset image ids between versions appart\n",
    "    \n",
    "    pbar = tqdm(flickr_ids_add)\n",
    "    for flickr_id in pbar:\n",
    "        file_name = f'{ID_PREFIX}{str(count_id).zfill(9)}.jpg'\n",
    "        response = json.loads(flickr.photos.getInfo(photo_id=flickr_id, format=\"json\"))\n",
    "        if response.get(\"photo\", True):\n",
    "            if int(response[\"photo\"][\"license\"]) in allowed_licenses:\n",
    "                url = get_url(flickr_id, 'z')\n",
    "                download_image(url, img_dir, file_name)\n",
    "                remove_metadata_img(Path(img_dir, file_name), quality=98)  # quality could enlarge image size denpending on flickr image quality\n",
    "                width, height = Image.open(Path(img_dir, file_name)).size\n",
    "                image_meta.append(\n",
    "                    {\n",
    "                        \"license\": response[\"photo\"][\"license\"],\n",
    "                        \"file_name\": file_name,\n",
    "                        \"dataset_version\": version,\n",
    "                        \"height\": height,\n",
    "                        \"width\": width,\n",
    "                        \"date_captured\": response[\"photo\"][\"dates\"][\"taken\"],\n",
    "                        \"flickr_url\": url,\n",
    "                        \"content_url\": response[\"photo\"][\"urls\"][\"url\"][0][\"_content\"],\n",
    "                        \"flickr_id\": response[\"photo\"][\"id\"],\n",
    "                        \"id\": int(f'{ID_PREFIX}{str(count_id).zfill(9)}')\n",
    "                    }\n",
    "                )\n",
    "            else:\n",
    "                print(f'License Error: image {flickr_id} not added; license {response[\"photo\"][\"license\"]} not allowed.')\n",
    "        else:\n",
    "            print(f'Error for flickr_id{flickr_id}: json response does not contain a photo item \\n json response: \\n')\n",
    "            pprint(response)   \n",
    "        count_id+=1\n",
    "        pbar.set_description(f'download+annotate: image: {flickr_id}')\n",
    "        \n",
    "    changes = True if len(image_meta) > 0 else False\n",
    "    final_anno[\"images\"] += image_meta\n",
    "\n",
    "    # download remaining\n",
    "    if len(flickr_ids_download) > 0:\n",
    "        pbar_2 = tqdm(flickr_ids_download) \n",
    "        for flickr_id in pbar_2:\n",
    "            file_name = next((item for item in final_anno[\"images\"] if item[\"flickr_id\"] == flickr_id), None)[\"file_name\"]\n",
    "            url = get_url(flickr_id, 'z')\n",
    "            download_image(url, img_dir, file_name)\n",
    "            remove_metadata_img(Path(img_dir, file_name))\n",
    "            pbar_2.set_description(f'download: image: {flickr_id}')\n",
    "    \n",
    "    # finish and save annotation file\n",
    "    final_anno[\"info\"][\"version\"] = version\n",
    "    save_file = f'annotationMeta_v{version}.json'\n",
    "    if anno_file:\n",
    "        if changes:\n",
    "            save_path = Path(Path(anno_file).parent, save_file)\n",
    "            if save_path.is_file():  \n",
    "                print(f'Error: {save_file} already exists.')\n",
    "            else:\n",
    "                with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                    json.dump(final_anno, f, ensure_ascii=False, indent=4)\n",
    "        else:\n",
    "            print('Warning: annotation file says: nothing changed')\n",
    "    else:\n",
    "        save_path = Path(img_dir, save_file)\n",
    "        if save_path.is_file():  \n",
    "            print(f'Error: {save_file} already exists in image directory.')\n",
    "        else:\n",
    "            with open(save_path, 'w', encoding='utf-8') as f:\n",
    "                json.dump(final_anno, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "\n",
    "## helper functions\n",
    "\n",
    "def get_url(id:str, size_suffix:str=''):\n",
    "    \"\"\"Gets URL for given image id and size.\n",
    "\n",
    "    args:\n",
    "    id: flickr image id\n",
    "    size: defines image size with suffix: https://www.flickr.com/services/api/misc.urls.html\n",
    "    \"\"\"\n",
    "    get_sizes = json.loads(flickr.photos.getSizes(photo_id=id, format=\"json\"))\n",
    "    found = False\n",
    "    last_size_url = None\n",
    "    for size in get_sizes['sizes']['size']:\n",
    "        last_size_url = size['source']\n",
    "        _size_suffix = size['source'].split('_')[-1].split('.')[0]  # gets (size) suffix from image source link\n",
    "        if _size_suffix == size_suffix:\n",
    "            url = size['source']\n",
    "            found = True\n",
    "            break\n",
    "        elif len(_size_suffix) > 1 and size_suffix == '':  # standard format without suffix in link\n",
    "            url = size['source']\n",
    "            found = True\n",
    "            break\n",
    "    if not found:   \n",
    "        url = last_size_url\n",
    "        suffix = last_size_url.split('_')[-1].split('.')[0]\n",
    "        print(f'Link/size not found for {id}; biggest image URL available saved, suffix: {suffix if len(suffix)==1 else \"none (default)\"}')\n",
    "    return url\n",
    "\n",
    "def download_image(url:str, output_dir:str, file_name:str):\n",
    "    \"\"\"Donwloads image from URL.\n",
    "    \"\"\"\n",
    "    Path(output_dir).mkdir(parents=True, exist_ok=True)\n",
    "    urllib.request.urlretrieve(url, Path(output_dir, file_name))\n",
    "    \n",
    "def remove_metadata_img(img_path:str, quality=75):\n",
    "    \"\"\"Removes all metadata of an image. Will also compress the image (PIL default is quality=75).\n",
    "    \"\"\"\n",
    "    image = Image.open(img_path)\n",
    "    data = list(image.getdata())\n",
    "    img_no_meta = Image.new(image.mode, image.size)\n",
    "    img_no_meta.putdata(data)\n",
    "    img_no_meta.save(img_path, quality=quality)  # default quality=75\n",
    "    \n",
    "def save_coco(img_path:str, quality=75):\n",
    "    \"\"\"Remove meta and resize.\n",
    "    \"\"\"\n",
    "    image = Image.open(img_path)\n",
    "    image.thumbnail((640, 640), Image.Resampling.LANCZOS)  # rescale to 640 max. in either dimension\n",
    "    data = list(image.getdata())  # list of raw pixel data\n",
    "    img_no_meta = Image.new(image.mode, image.size)\n",
    "    img_no_meta.putdata(data)\n",
    "    img_no_meta.save(img_path, quality=quality)  # default quality=75\n",
    "\n",
    "def ids_from_files(dir):\n",
    "    \"\"\"Retrieve flickr IDs from the file name of downloaded images.\n",
    "    \n",
    "    flickr file name format: numericalID_imageSecret_imageSize.jpg, e.g. 50235154168_b3201cd930_o.jpg\n",
    "    \"\"\"\n",
    "    return set([x.name.split('_')[0] for x in Path(dir).glob(\"*.jpg\")])\n",
    "\n",
    "\n",
    "# ids = ids_from_files('/Users/john/Downloads/fireground_dataset_upgrade')\n",
    "# test_ids = ['4882517696', '4882517696']\n",
    "# build_dataset(ids, '/Users/john/Downloads/fireground_dataset', '/Users/john/Downloads/fireground_dataset/annotationMeta_v1.0.json')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e74c16-b112-4849-9a81-730669ee36d3",
   "metadata": {},
   "source": [
    "## Merge coco-annotator export with metadata annotation file created above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "id": "205d31fd-8cd9-422b-afc1-016dc9acf99e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# After annotating in coco-annotator, add the annotations to the annotation file created above\n",
    "\n",
    "# keys in annotations to be removed\n",
    "ANNO_DELETE = [\"color\", \"metadata\"]\n",
    "\n",
    "\n",
    "def add_coco_annotations(anno_meta, anno_coco):\n",
    "    \"\"\"\n",
    "    Adds in external tools created annotations to the annotation file created above. \n",
    "    The file names must remain the same during the whole process.\n",
    "    Image ids are drawn from the anno_meta file.\n",
    "    \n",
    "    args:\n",
    "    anno_meta: path to .json file – contains meta information for a dataset, e.g., created above with build dataset\n",
    "    anno_coco: path to .json file – annotation export from coco-annotator\n",
    "    \n",
    "    TODO: cleanup bounding box, set to max image size\n",
    "    \"\"\"\n",
    "    with open(anno_meta) as f:\n",
    "        meta_json = json.load(f)\n",
    "    with open(anno_coco) as f:\n",
    "        coco_json = json.load(f)\n",
    "        \n",
    "    # build lookup tables with file names and image ids\n",
    "    key_fn_id_meta = {}\n",
    "    key_id_fn_coco = {}\n",
    "    for image in meta_json[\"images\"]:\n",
    "        key_fn_id_meta[image[\"file_name\"]] = image[\"id\"]\n",
    "    for image in coco_json[\"images\"]:\n",
    "        key_id_fn_coco[image[\"id\"]] = image[\"file_name\"]\n",
    "    # keep category id from meta_json\n",
    "    # category name has to be the same\n",
    "    key_name_cat_meta = {}\n",
    "    key_cat_name_coco = {}\n",
    "    for category in meta_json[\"categories\"]:\n",
    "        key_name_cat_meta[category[\"name\"]] = category[\"id\"]\n",
    "    for category in coco_json[\"categories\"]:\n",
    "        key_cat_name_coco[category[\"id\"]] = category[\"name\"]\n",
    "        \n",
    "    annotations = []\n",
    "    for anno in coco_json[\"annotations\"]:\n",
    "        file_name = key_id_fn_coco[anno[\"image_id\"]]\n",
    "        id_meta = key_fn_id_meta[file_name]\n",
    "        category_name = key_cat_name_coco[anno[\"category_id\"]]\n",
    "        category_id_meta = key_name_cat_meta[category_name]\n",
    "        \n",
    "        # annotation items to keep\n",
    "        annotation = {}\n",
    "        for key, value in anno.items():\n",
    "            if key not in ANNO_DELETE:\n",
    "                annotation[key] = value\n",
    "        \n",
    "        # overwrite id keys with anno_meta values\n",
    "        annotation[\"id\"] = anno[\"id\"]\n",
    "        annotation[\"image_id\"] = id_meta\n",
    "        annotation[\"category_id\"] = category_id_meta\n",
    "        \n",
    "        # finally add new annotation\n",
    "        annotations.append(annotation)\n",
    "        \n",
    "    # merge and save\n",
    "    final_anno = meta_json.copy()\n",
    "    final_anno[\"annotations\"] = annotations\n",
    "    version = meta_json[\"info\"][\"version\"]\n",
    "    save_file = f'annotations_v{version}.json'\n",
    "    save_path = Path(Path(anno_meta).parent, save_file)\n",
    "    if save_path.is_file():  \n",
    "        print(f'Error: {save_file} already exists.')\n",
    "    else:\n",
    "        with open(save_path, 'w', encoding='utf-8') as f:\n",
    "            json.dump(final_anno, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "            \n",
    "add_coco_annotations('/Users/john/datasets/fireground_pose/json/archive/annotationMeta_v2.0.json', \n",
    "                     '/Users/john/Downloads/json.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd2d72b0-d13b-4b90-bd5d-b41922b7f301",
   "metadata": {},
   "source": [
    "## Check annotations in fiftyone"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "66837041-c20f-40be-bf3e-53ddc1554cc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%capture\n",
    "# check annotations in fiftyone\n",
    "import fiftyone as fo\n",
    "import webbrowser\n",
    "\n",
    "# dataset = fo.Dataset.from_dir(\n",
    "#     dataset_type = fo.types.COCODetectionDataset,\n",
    "#     label_types = [\"detections\", \"segmentations\", \"keypoints\"],\n",
    "#     data_path = f'{cfg.DATASET.CROWD_POSE.ROOT}/images',\n",
    "#     labels_path = f'{cfg.DATASET.CROWD_POSE.ROOT}/json/crowdpose_val.json',\n",
    "#     max_samples=2000\n",
    "# )\n",
    "\n",
    "# dataset = fo.Dataset.from_dir(\n",
    "#     dataset_type = fo.types.COCODetectionDataset,\n",
    "#     label_types = [\"detections\", \"segmentations\", \"keypoints\"],\n",
    "#     data_path = f'{cfg.DATASET.COCO.ROOT}/images/val2017',\n",
    "#     labels_path = f'{cfg.DATASET.COCO.ROOT}/annotations/person_keypoints_val2017.json',\n",
    "#     max_samples=2000\n",
    "# )\n",
    "\n",
    "# dataset = fo.Dataset.from_dir(\n",
    "#     dataset_type = fo.types.COCODetectionDataset,\n",
    "#     label_types = [\"detections\", \"segmentations\", \"keypoints\"],\n",
    "#     data_path = f'{cfg.DATASET.MYIMG.ROOT}/images',\n",
    "#     labels_path = f'{cfg.DATASET.MYIMG.ROOT}/json/archive/json4.json',\n",
    "#     max_samples=20000\n",
    "# )\n",
    "\n",
    "dataset = fo.Dataset.from_dir(\n",
    "    dataset_type = fo.types.COCODetectionDataset,\n",
    "    label_types = [\"detections\", \"segmentations\", \"keypoints\"],\n",
    "    data_path = f'{cfg.DATASET.MYIMG.ROOT}/images',\n",
    "    labels_path = f'{cfg.DATASET.MYIMG.ROOT}/json/annotations_crowd.json',\n",
    "    max_samples=20000\n",
    ")\n",
    "\n",
    "# dataset = fo.Dataset.from_dir(\n",
    "#     dataset_type = fo.types.COCODetectionDataset,\n",
    "#     label_types = [\"detections\", \"segmentations\", \"keypoints\"],\n",
    "#     data_path = f'{cfg.DATASET.MYIMG.ROOT}/images',\n",
    "#     labels_path = f'{cfg.DATASET.MYIMG.ROOT}/json/annotations/annotations_coco.json',\n",
    "#     max_samples=20000\n",
    "# )\n",
    "\n",
    "port = 5151\n",
    "session = fo.launch_app(port=port)\n",
    "webbrowser.open(f'http://localhost:{port}/')\n",
    "\n",
    "dataset.persistent = True\n",
    "session.view = dataset.view()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9d007d9b-ac33-4b33-a417-b0693f7454aa",
   "metadata": {},
   "source": [
    "# some useful functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "2cac2e16-628c-4622-916b-80f519556e5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_key(container:dict):\n",
    "    \"\"\"Drop dict key&value on all levels; e.g., segmentations in an annotation file.\n",
    "    \"\"\"\n",
    "    if not isinstance(container, dict):\n",
    "        return container if not isinstance(container, list) else list(map(d_rem, container))\n",
    "    return {a:d_rem(b) for a, b in container.items() if a != 'segmentation'}  # adjust key\n",
    "\n",
    "\n",
    "json_path = '/Users/john/git/_tools/coco-annotator/datasets/annotations_coco_person/coco_person_keypoints_val2017_segDel.json'\n",
    "save_path = '/Users/john/git/_tools/coco-annotator/datasets/annotations_coco_person/coco_person_keypoints_val2017_segDel2.json'\n",
    "\n",
    "# with open(json_path) as f:\n",
    "#     b = remove_key(json.load(f))\n",
    "# with open(save_path, 'w', encoding='utf-8') as f:\n",
    "#     json.dump(b, f, ensure_ascii=False, indent=4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "9eb9e16f-4b9f-4967-ba78-f8d212211502",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Switch value of two keys in dict\n",
    "\n",
    "path = '/Users/john/Downloads/fireground_dataset/annotationMeta_v1.0.json'\n",
    "with open(path) as f:\n",
    "    old = json.load(f)\n",
    "    new = old.copy()\n",
    "    new.pop('images')\n",
    "    new['images'] = []\n",
    "    \n",
    "    for image in old['images']:\n",
    "        img = image.copy()\n",
    "        img['height'] = image['width']\n",
    "        img['width'] = image['height']\n",
    "        new['images'].append(img)\n",
    "      \n",
    "    # with open(Path(Path(path).parent, 'annotationMeta_v1.1.json'), 'w', encoding='utf-8') as f:\n",
    "    #             json.dump(new, f, ensure_ascii=False, indent=4)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "13910416-6756-443a-aec4-4b9b63c06ceb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "count_crowd=12234\n",
      "1089\n",
      "count_coco=3238\n",
      "1089\n",
      "kp_unique=15472\n"
     ]
    }
   ],
   "source": [
    "# count keypoints / people\n",
    "\n",
    "path1 = f'{cfg.DATASET.MYIMG.ROOT}/json/annotations_crowd.json'\n",
    "path2 = f'{cfg.DATASET.MYIMG.ROOT}/json/annotations_coco.json'\n",
    "\n",
    "count_crowd = 0\n",
    "with open(path1) as f:\n",
    "    file = json.load(f)\n",
    "    for anno in file['annotations']:\n",
    "        for kp_vis in anno['keypoints'][2::3]:  # loops for every keypoint through visibility index\n",
    "            if kp_vis > 0:\n",
    "                count_crowd += 1\n",
    "    print(f'{count_crowd=}')\n",
    "    print(len(file['annotations']))\n",
    "    \n",
    "count_coco = 0\n",
    "with open(path2) as f:\n",
    "    file = json.load(f)\n",
    "    for anno in file['annotations']:\n",
    "        for kp_vis in anno['keypoints'][2:5*3:3]:  # loops for every keypoint through visibility index; ONLY for new HEAD keypoints (first 5)\n",
    "            if kp_vis > 0:\n",
    "                count_coco += 1\n",
    "    print(f'{count_coco=}')\n",
    "    print(len(file['annotations']))\n",
    "                \n",
    "print(f'kp_unique={count_crowd+count_coco}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81772583-670e-4db3-8ec4-cdd0fc6da0ea",
   "metadata": {},
   "source": [
    "## Add approximate bounding boxes & segmentation around keypoints for adjustment in coco-annotator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "id": "d8ccc5a8-1227-454c-b405-f5d3a8eee8de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add bounding box to keypoints for later adjustment in coco annotator\n",
    "# will override old bounding box and segmentation for annotation, adjust code if not wanted\n",
    "\n",
    "path = f'{cfg.DATASET.MYIMG.ROOT}/json/crowdpose_test.json'\n",
    "with open(path) as f:\n",
    "    old = json.load(f)\n",
    "    new = old.copy()\n",
    "    new.pop('annotations')\n",
    "    new['annotations'] = []\n",
    "    \n",
    "    # lookup table for image dimensions\n",
    "    lookup = {image['id']: {'height': image['height'], 'width': image['width']} for image in old['images']}\n",
    "    \n",
    "    for anno in old['annotations']:\n",
    "        _anno = anno.copy()\n",
    "        image_id = anno['image_id']\n",
    "        x_max = lookup[image_id]['width']\n",
    "        y_max = lookup[image_id]['height']\n",
    "        kps = anno['keypoints']\n",
    "        # delete not annotated keypoints (v=0)\n",
    "        kps = np.array_split(kps, len(kps)/3)\n",
    "        kps = list(np.array([kp for kp in kps if kp[2] > 0]).flatten())\n",
    "        # define bounding box\n",
    "        xs = kps[0::3]\n",
    "        ys = kps[1::3]\n",
    "        bbox = [min(xs), min(ys), max(xs)-min(xs), max(ys)-min(ys)]\n",
    "        # give some margins\n",
    "        margin = 10. # px, all directions\n",
    "        bbox = [bbox[0]-margin, bbox[1]-margin, bbox[2]+2*margin, bbox[3]+2*margin]\n",
    "        bbox = [int(i) for i in bbox]\n",
    "        # set to image edge if out of bounce\n",
    "        bbox[0] = bbox[0] if bbox[0] > 0 else 0\n",
    "        bbox[1] = bbox[1] if bbox[1] > 0 else 0\n",
    "        bbox[2] = bbox[2] if bbox[0]+bbox[2] < x_max else x_max-bbox[0]\n",
    "        bbox[3] = bbox[3] if bbox[1]+bbox[3] < y_max else y_max-bbox[1]\n",
    "        # coco annotator binds a 4 corner segmentation to the bounding box: to be able to adjust the bbox, the segmentation is also needed\n",
    "        segmentation = [bbox[0]+bbox[2], bbox[1], bbox[0]+bbox[2], bbox[1]+bbox[3], bbox[0], bbox[1]+bbox[3], bbox[0], bbox[1]]\n",
    "        segmentation = [float(val) for val in segmentation]\n",
    "        \n",
    "        _anno['area'] = bbox[2] * bbox[3]\n",
    "        _anno['bbox'] = bbox\n",
    "        _anno['segmentation'] = [segmentation]\n",
    "        _anno['isbbox'] = True  # use segmentation as bounding box in coco_annotator\n",
    "        \n",
    "        new['annotations'].append(_anno)\n",
    "        \n",
    "    with open(Path(Path(path).parent, 'update.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(new, f, ensure_ascii=False, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a8f259d-413f-4941-806f-9742411b0ec8",
   "metadata": {},
   "source": [
    "## Coco-export cleanup for bounding boxes (fix max. dimension) and unused keys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "id": "384019d1-39fa-4d2c-89e3-f11e36747b4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# clean&fix coco-annotator export\n",
    "# remove unused keys like area & segmentation\n",
    "# adjust bounding boxes to max. image dimensions\n",
    "\n",
    "path = f'/Users/john/Downloads/fp_6-11.json'\n",
    "with open(path) as f:\n",
    "    old = json.load(f)\n",
    "    new = old.copy()\n",
    "    new.pop('annotations')\n",
    "    new['annotations'] = []\n",
    "    \n",
    "    # lookup table for image dimensions\n",
    "    lookup = {image['id']: {'height': image['height'], 'width': image['width']} for image in old['images']}\n",
    "    \n",
    "    for anno in old['annotations']:\n",
    "        _anno = {}\n",
    "        image_id = anno['image_id']\n",
    "        x_max = lookup[image_id]['width']\n",
    "        y_max = lookup[image_id]['height']\n",
    "        # keep\n",
    "        _anno['id'] = anno['id']\n",
    "        _anno['image_id'] = anno['image_id']\n",
    "        _anno['category_id'] = anno['category_id']\n",
    "        _anno['iscrowd'] = anno['iscrowd']\n",
    "        try:\n",
    "            _anno['keypoints'] = anno['keypoints']\n",
    "            _anno['num_keypoints'] = anno['num_keypoints']\n",
    "        except KeyError:\n",
    "            _anno['keypoints'] = [0] * 14 * 3  # 14 keypoints, all values 0\n",
    "            _anno['num_keypoints'] = 0 \n",
    "        # adjust bbox to max. image dimensions\n",
    "        bbox = anno['bbox']\n",
    "        bbox[0] = bbox[0] if bbox[0] > 0 else 0\n",
    "        bbox[1] = bbox[1] if bbox[1] > 0 else 0\n",
    "        bbox[2] = bbox[2] if bbox[0]+bbox[2] < x_max else x_max-bbox[0]\n",
    "        bbox[3] = bbox[3] if bbox[1]+bbox[3] < y_max else y_max-bbox[1]\n",
    "        _anno['bbox'] = bbox\n",
    "        \n",
    "        new['annotations'].append(_anno)\n",
    "        \n",
    "    # export\n",
    "    with open(Path(Path(path).parent, 'json.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(new, f, ensure_ascii=False, indent=4)   \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3562d6bd-5bce-4a9a-8a5d-3bfdfbc71e70",
   "metadata": {},
   "source": [
    "## Add crowdIndex & num_people to annotation file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "2b78c6b4-807d-44f3-8952-e34695f56e4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Update annotation file with crowdIndex & num_people\n",
    "# https://github.com/Jeff-sjtu/CrowdPose\n",
    "# Note: crowdIndex is sometimes wrong in the original dataset; num_keypoints always lacks 2 (presumably head, neck)\n",
    "\n",
    "def point_in_rect(point, rect):\n",
    "    \"\"\"Check if point is in rectangle.\"\"\"\n",
    "    x1, y1, w, h = rect\n",
    "    x2, y2 = x1+w, y1+h\n",
    "    x, y = point\n",
    "    if (x1 < x and x < x2):\n",
    "        if (y1 < y and y < y2):\n",
    "            return True\n",
    "    return False\n",
    "\n",
    "# path = f'{cfg.DATASET.CROWD_POSE.ROOT}/json/crowdpose_val.json'\n",
    "path = f'{cfg.DATASET.MYIMG.ROOT}/json/archive/json3.json'\n",
    "with open(path) as f:\n",
    "    old = json.load(f)\n",
    "    new = old.copy()\n",
    "    \n",
    "    anno_lookup = {}\n",
    "    for image in old['images']:\n",
    "        annos = []\n",
    "        for anno in old['annotations']:\n",
    "            if anno['image_id'] == image['id']:\n",
    "                annos.append(anno)\n",
    "        anno_lookup[image['id']] = annos\n",
    "    \n",
    "    crowdedness_all = {}\n",
    "    for _image_id, _annos in anno_lookup.items():\n",
    "        bboxes = {}\n",
    "        keypoints = {}\n",
    "        num_keypoints = {}\n",
    "        num_people = {}\n",
    "        crowdedness = {}\n",
    "        ids = [anno['id'] for anno in _annos if not anno['iscrowd']]  # skip iscrowd instances (contains several people at once)\n",
    "        for anno in _annos:\n",
    "            bboxes[anno['id']] = anno['bbox']\n",
    "            kps = anno['keypoints']\n",
    "            kps = np.array_split(kps, len(kps)/3)\n",
    "            kps = [list(kp) for kp in kps if kp[2] > 0]  # count all annotated keypoints, also hidden ones\n",
    "            keypoints[anno['id']] = kps\n",
    "            num_keypoints[anno['id']] = len(kps)\n",
    "        for idx in ids:  # for each annotation instance\n",
    "            bbox = bboxes[idx]\n",
    "            kp_count = {'kps_self': 0, 'kps_other': 0}\n",
    "            for image_id, kps in keypoints.items():  # check all keypoints of all annotations\n",
    "                if idx == image_id:  # keypoints are in bbox of instance itself\n",
    "                    for kp in kps:\n",
    "                        if point_in_rect((kp[0], kp[1]), bbox):\n",
    "                            kp_count['kps_self'] += 1\n",
    "                else:\n",
    "                    for kp in kps:\n",
    "                        if point_in_rect((kp[0], kp[1]), bbox):\n",
    "                            kp_count['kps_other'] += 1\n",
    "            crowdedness[idx] = kp_count\n",
    "        crowdedness_all[_image_id] = crowdedness\n",
    "    # calculate crowdIndex\n",
    "    crowdIndex = {}\n",
    "    for image_id, kps_stats in crowdedness_all.items():\n",
    "        ci_sum = 0\n",
    "        num_valid = 0\n",
    "        num_persons = 0\n",
    "        for _, stats in kps_stats.items():\n",
    "            try:\n",
    "                ci_sum += stats['kps_other']/stats['kps_self']\n",
    "                num_valid += 1\n",
    "                num_persons += 1\n",
    "            except ZeroDivisionError:\n",
    "                # skip people with 0 keypoints for index calculation\n",
    "                num_persons += 1\n",
    "        crowdIndex[image_id] = ci_sum/num_valid\n",
    "        num_people[image_id] = num_persons\n",
    "    \n",
    "    # sort + normalize\n",
    "    # !! original crowd_pose dataset contains annotation errors end outliers shift the crowdIndex to not usable values\n",
    "    _max = crowdIndex[max(crowdIndex, key=crowdIndex.get)]\n",
    "    crowdIndex = {k: v/_max for k, v in sorted(crowdIndex.items(), key=lambda item: item[1], reverse=True)}\n",
    "    \n",
    "    # build json\n",
    "    new.pop('images')\n",
    "    new.pop('annotations')\n",
    "    new['images'] = []\n",
    "    for image in old['images']:  \n",
    "        # define (new) order\n",
    "        _image = {}\n",
    "        _image['license'] = image['license']\n",
    "        _image['file_name'] = image['file_name']\n",
    "        _image['dataset_version'] = image['dataset_version']\n",
    "        _image['width'] = image['width']\n",
    "        _image['height'] = image['height']\n",
    "        _image['num_people'] = num_people[image['id']]\n",
    "        _image['crowdIndex'] = round(crowdIndex[image['id']], 2)\n",
    "        _image['date_captured'] = image['date_captured']\n",
    "        _image['flickr_url'] = image['flickr_url']\n",
    "        _image['content_url'] = image['content_url']\n",
    "        _image['flickr_id'] = image['flickr_id']\n",
    "        _image['id'] = image['id']\n",
    "        \n",
    "        new['images'].append(_image)\n",
    "    new['annotations'] = old['annotations']  # keep annotations at bottom of json\n",
    "    \n",
    "    # with open(Path(Path(path).parent, 'json4.json'), 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(new, f, ensure_ascii=False, indent=4)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5188218-45d8-4b15-8303-6d4f89f18b48",
   "metadata": {},
   "source": [
    "## Add segmentation & area based on bounding boxes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "d19c6188-8e59-45f6-972b-976d0ee467e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# needed to calculate OKS in COCO\n",
    "# also needed in order for coco annotator to import bounding boxes\n",
    "# will not have meaningfull results, because those do not represent real ocject segmentations\n",
    "\n",
    "path = f'/Users/john/datasets/fireground_pose/json/annotations/annotations_coco.json'\n",
    "with open(path) as f:\n",
    "    _anno = json.load(f)\n",
    "    \n",
    "    for anno in _anno['annotations']:\n",
    "        b = anno['bbox']\n",
    "        # [x2, y2, x3, y3, x4, y4, x0, y0]\n",
    "        anno['segmentation'] = [ [b[0]+b[2], b[1], b[0]+b[2], b[1]+b[3], b[0], b[1]+b[3], b[0], b[1]] ]\n",
    "        anno['area'] = b[2]*b[3]\n",
    "        anno['isbbox'] = True  # for coco-annotator import, to bind segmentation to bounding box\n",
    "        \n",
    "    with open(Path(Path(path).parent, 'test.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(_anno, f, ensure_ascii=False, indent=4)   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bdae37c-611f-4556-9858-9853adbce2af",
   "metadata": {},
   "source": [
    "## Build coco annotations out of crowd_pose annotation file\n",
    "Keep the 12 body keypoints - head keypoints will be empty and have to be added manually"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "id": "9d702dfa-af2b-4b29-a729-1fa854737d3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = f'{cfg.DATASET.MYIMG.ROOT}/json/crowdpose_test.json'\n",
    "with open(path) as f:\n",
    "    old = json.load(f)\n",
    "    new = old.copy()\n",
    "    \n",
    "    # override with coco meta\n",
    "    new['categories'] = CATEGORIES_COCO\n",
    "    # remove what isn't valid anymore\n",
    "    for image in new['images']:\n",
    "        image.pop('crowdIndex')\n",
    "    # transfer keypoints\n",
    "    for annotation in new['annotations']:\n",
    "        # coco-annotator will not import empty persons; there, people where only head & neck are annotated need a fix\n",
    "        if sum(annotation['keypoints'][0:-2*3]) == 0:  # all keypoints zero besides head & neck\n",
    "            annotation['keypoints'] = annotation['keypoints'][-6:-3] + [0] * 4 * 3 + annotation['keypoints'][0:-2*3]  # same as below, but use head values as nose\n",
    "        else: \n",
    "        # insert 5 empty keypoints at the start (5*3 values, all zero)  + remove last 6 values (head & neck)\n",
    "            annotation['keypoints'] = [0] * 5 * 3 + annotation['keypoints'][0:-2*3]\n",
    "    \n",
    "    # save\n",
    "    # with open(Path(Path(path).parent, 'crowd_to_coco.json'), 'w', encoding='utf-8') as f:\n",
    "    #     json.dump(new, f, ensure_ascii=False, indent=4)   \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564506f6-d6b5-43df-a35e-b3c6f581776a",
   "metadata": {},
   "source": [
    "### Merge coco-annotator-coco export with crowd_pose file\n",
    "Use the same body keypoints as in the original crowd_pose annotations and add coco style head keypoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "1abe6152-ed40-4b10-8d76-45a8b71cdf1a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1089 1089\n"
     ]
    }
   ],
   "source": [
    "# anno IDs will have changed but are in the same order per image\n",
    "\n",
    "\n",
    "coco_path = f'/Users/john/Downloads/fp_8_coco-13.json'\n",
    "base_path = f'{cfg.DATASET.MYIMG.ROOT}/json/archive/annotations_v2.0.json'\n",
    "\n",
    "with open(coco_path, 'r') as coco, open(base_path, 'r') as base:\n",
    "    coco = json.load(coco)\n",
    "    base = json.load(base)\n",
    "    new = base.copy()\n",
    "    \n",
    "    # annotations with same order but different id\n",
    "    # check annotations per image and adjust in coco-annotator if necessary\n",
    "    coco_anno_lookup = {}\n",
    "    for image in coco['images']:\n",
    "        annos = []\n",
    "        for anno in coco['annotations']:\n",
    "            if anno['image_id'] == image['id']:\n",
    "                annos.append(anno)\n",
    "        coco_anno_lookup[image['id']] = annos\n",
    "        \n",
    "    base_anno_lookup = {}\n",
    "    for image in base['images']:\n",
    "        annos = []\n",
    "        for anno in base['annotations']:\n",
    "            if anno['image_id'] == image['id']:\n",
    "                annos.append(anno)\n",
    "        base_anno_lookup[image['id']] = annos\n",
    "        \n",
    "    for (k1,v1), (k2,v2) in zip(base_anno_lookup.items(), coco_anno_lookup.items()):\n",
    "        if len(v1) == len(v2):\n",
    "            pass\n",
    "        else:\n",
    "            print(f'Anno mismatch: {k1, k2}')\n",
    "    \n",
    "    print(len(coco['annotations']), len(base['annotations']))\n",
    "    \n",
    "    # merge\n",
    "    for anno1, anno2 in zip(new['annotations'], coco['annotations']):\n",
    "        # print(anno2['keypoints'])\n",
    "        try:\n",
    "            anno1['keypoints'] = anno2['keypoints'][0:5*3] + anno1['keypoints'][0:-2*3]\n",
    "        except KeyError:  # no keypoints in coco annotation (should already be fixed in base file)\n",
    "            anno1['keypoints'] = [0] * 3 * 5 + anno1['keypoints'][0:-2*3]\n",
    "    \n",
    "    # remove crowdIndex (needs new calculation due to new keypoint schema)\n",
    "    for image in new['images']:\n",
    "        image.pop('crowdIndex')\n",
    "        \n",
    "    # override header (coco keypoint schema)\n",
    "    new['categories'] = CATEGORIES_COCO\n",
    "    \n",
    "    with open(Path(Path(base_path).parent, 'json3.json'), 'w', encoding='utf-8') as f:\n",
    "        json.dump(new, f, ensure_ascii=False, indent=4)\n",
    "    \n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
